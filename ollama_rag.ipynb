{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f07a80ee-d6d2-470a-b63a-bccc3cc97ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_experimental\n",
    "# langchain_openai\n",
    "# !pip install rank_bm25\n",
    "# !pip install yandexcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4826a0-47b0-4c1e-91ae-af00bb04d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278d0bee-c9ba-4edb-9762-fc4fde9a3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "# from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Tuple, Optional\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ca2105-857a-49aa-9d01-36d60bfb8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_Tinkoff/dataset.json', 'r') as f:\n",
    "    f = json.loads(f.read())\n",
    "\n",
    "for i in range(len(f['data'])):\n",
    "    f['data'][i]['id_doc'] = i\n",
    "\n",
    "documents_raw_dict = {}\n",
    "for i in range(len(f['data'])):\n",
    "    documents_raw_dict[f['data'][i]['id_doc']] = f['data'][i]['description']\n",
    "\n",
    "with open('dataset.json', 'w') as fout:\n",
    "    json.dump(f['data'] , fout)\n",
    "\n",
    "def metadata_func(content: dict, metadata: dict) -> dict:\n",
    "    metadata['id_doc'] = content.get('id_doc')\n",
    "    metadata[\"title\"] = content.get(\"title\")\n",
    "    metadata[\"parent_title\"] = content.get(\"parent_title\")\n",
    "    metadata[\"business_line_id\"] = content.get(\"business_line_id\")\n",
    "    metadata[\"direction\"] = content.get(\"direction\")\n",
    "    metadata[\"product\"] = content.get(\"product\")\n",
    "    metadata[\"type\"] = content.get(\"type\")\n",
    "    metadata[\"url\"] = content.get(\"url\")\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./dataset.json',\n",
    "    jq_schema='.[]',\n",
    "    content_key='description',\n",
    "    metadata_func=metadata_func)\n",
    "\n",
    "documents = loader.load()[:100]\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    documents[i].page_content = documents[i].metadata['title'] + ' ' + documents[i].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0351df22-05c1-42ff-97c8-75165f258358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3e75f57f-3f6f-4eaf-ae5d-ec6586d201f1',\n",
       " 'title': 'Как посмотреть лимиты счета?',\n",
       " 'url': 'https://www.tinkoff.ru/business/help/business-payout/jump/about/summary/?card=q1',\n",
       " 'source': 'web',\n",
       " 'business_line_id': 'business',\n",
       " 'direction': 'business-payout',\n",
       " 'product': 'jump',\n",
       " 'type': 'card',\n",
       " 'description': 'Вы можете посмотреть лимиты по счету на месяц, по карте на месяц и на разовую выплату. Для этого: Откройте раздел «Сводка». В разделе «Счета» найдите нужное вам юридическое лицо и нажмите на него. В открывшемся окне будут представлены все расчетные счета, к которым есть доступ с этого юридического лица. Найдите нужный вам счет.',\n",
       " 'parent_title': 'Сводка',\n",
       " 'parent_url': 'https://www.tinkoff.ru/business/help/business-payout/jump/about/summary/',\n",
       " 'id_doc': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c978197b-569e-4351-be71-0e3ecafe7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_documents(\n",
    "#     chunk_size: int,\n",
    "#     knowledge_base: List[LangchainDocument],\n",
    "# ) -> List[LangchainDocument]:\n",
    "#     \"\"\"\n",
    "#     Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "#     # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "#     MARKDOWN_SEPARATORS = [\n",
    "#         \"\\n#{1,6} \",\n",
    "#         \"```\\n\",\n",
    "#         \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "#         \"\\n---+\\n\",\n",
    "#         \"\\n___+\\n\",\n",
    "#         \"\\n\\n\",\n",
    "#         \"\\n\",\n",
    "#         \" \",\n",
    "#         \"\",\n",
    "#     ]\n",
    "    \n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=int(chunk_size / 10),\n",
    "#         add_start_index=True,\n",
    "#         strip_whitespace=True,\n",
    "#         separators=MARKDOWN_SEPARATORS,\n",
    "#     )\n",
    "\n",
    "#     docs_processed = []\n",
    "#     for doc in knowledge_base:\n",
    "#         docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "#     # Remove duplicates\n",
    "#     unique_texts = {}\n",
    "#     docs_processed_unique = []\n",
    "#     for doc in docs_processed:\n",
    "#         if doc.page_content not in unique_texts:\n",
    "#             unique_texts[doc.page_content] = True\n",
    "#             docs_processed_unique.append(doc)\n",
    "\n",
    "#     return docs_processed_unique\n",
    "\n",
    "\n",
    "# docs_processed = split_documents(\n",
    "#     1000,  # We choose a chunk size adapted to our model\n",
    "#     documents,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a467a5cf-b77d-4741-8aa0-da6e88b5e2c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "def split_documents(\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    embeddings\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "    # This list is taken from LangChain's MarkdownTextSplitter class\n",
    "    MARKDOWN_SEPARATORS = [\n",
    "        \"\\n#{1,6} \",\n",
    "        \"```\\n\",\n",
    "        \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "        \"\\n---+\\n\",\n",
    "        \"\\n___+\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings, breakpoint_threshold_type=\"percentile\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in tqdm(knowledge_base):\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='aya:8b')\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    documents,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "29206186-5bc3-4d34-ba52-a3632f76337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "03f13192-b17d-4d0d-bed4-670b81cee146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Как посмотреть лимиты счета?', metadata={'source': '/Users/hare/code2/tinkoff/dataset.json', 'seq_num': 1, 'id_doc': 0, 'title': 'Как посмотреть лимиты счета?', 'parent_title': 'Сводка', 'business_line_id': 'business', 'direction': 'business-payout', 'product': 'jump', 'type': 'card', 'url': 'https://www.tinkoff.ru/business/help/business-payout/jump/about/summary/?card=q1'})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca980b95-cb33-4bff-9c91-977d09ff02fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Как добавить пользователя в личный кабинет?', metadata={'source': '/Users/hare/code2/tinkoff/dataset.json', 'seq_num': 28, 'id_doc': 27, 'title': 'Как добавить пользователя в личный кабинет?', 'parent_title': 'Настройки', 'business_line_id': 'business', 'direction': 'business-payout', 'product': 'jump', 'type': 'card', 'url': 'https://www.tinkoff.ru/business/help/business-payout/jump/about/settings/?card=q1', '_id': 'c902f916c2964ee595c4db4186ea82b8', '_collection_name': 'my_documents'}),\n",
       " 0.120165473298223)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNOWLEDGE_VECTOR_DATABASE.similarity_search_with_score('dfdf')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f3f3cf02-c4ba-4b30-9c47-bcee2e721894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    knowledge_index: None,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 5,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "\n",
    "    llm = ChatOllama(model=\"aya:8b\")\n",
    "\n",
    "    # Give analogue questions\n",
    "    questions = fusion_questions(question, llm, 4)\n",
    "    print(questions)\n",
    "    print()\n",
    "\n",
    "    # Gather documents with retriever\n",
    "    print(\"=> Retrieving documents...\")\n",
    "\n",
    "    relevant = []\n",
    "    scores = []\n",
    "    for question in questions:\n",
    "        rels = knowledge_index.similarity_search_with_score(query=question, k=num_retrieved_docs)\n",
    "        for rel in rels:\n",
    "            relevant.append(rel[0])\n",
    "            scores.append(rel[1])\n",
    "        # relevant.extend(ensemble_retriever.invoke(question, k=num_retrieved_docs))\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    relevant_unique = []\n",
    "    scores_unique = []\n",
    "    for i, doc in enumerate(relevant):\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            relevant_unique.append(doc)\n",
    "            scores_unique.append(scores[i])\n",
    "    print(f\"=> Unique documents: {len(relevant_unique)}\")\n",
    "\n",
    "    relevant_docs = [doc for i, doc in enumerate(relevant_unique) if scores_unique[i] > np.quantile(scores_unique, 0.9)]\n",
    "    \n",
    "    # relevant_docs = [doc.page_content for doc in relevant_unique]  # Keep only the text\n",
    "\n",
    "    # print(relevant_unique)\n",
    "    \n",
    "    # # Optionally rerank results\n",
    "    # reranker_k = min(10, len(relevant_docs))\n",
    "    # if reranker:\n",
    "    #     print(\"=> Reranking documents...\")\n",
    "    #     relevant_docs = reranker.rerank(question, relevant_docs, k=reranker_k)\n",
    "\n",
    "    #     scores = [doc[\"score\"] for doc in relevant_docs]\n",
    "    #     print(np.quantile(scores, 0.9))\n",
    "        \n",
    "    #     relevant_docs = [doc['content'] for doc in relevant_docs if doc['score'] > np.quantile(scores, 0.9)]\n",
    "    #     print(relevant_docs)\n",
    "\n",
    "    # relevant_unique = [doc for doc in relevant_unique if doc.page_content in relevant_docs]\n",
    "\n",
    "    # relevant_unique_temp = []\n",
    "    # for content in relevant_docs:\n",
    "    #     for doc in relevant_unique:\n",
    "    #         if content == doc.page_content:\n",
    "    #             relevant_unique_temp.append(doc)\n",
    "\n",
    "    # print(relevant_unique_temp)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_urls = {}\n",
    "    relevant_unique_ = []\n",
    "    for doc in relevant_docs:\n",
    "        if doc.metadata['url']not in unique_urls:\n",
    "            unique_urls[doc.metadata['url']] = True\n",
    "            relevant_unique_.append(doc)\n",
    "\n",
    "    relevant_docs = [documents_raw_dict[x.metadata['id_doc']] for x in relevant_unique_]\n",
    "\n",
    "    print(len(relevant_docs))\n",
    "    print(relevant_docs)\n",
    "\n",
    "    context = \"\\nИзвлеченные документы:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "            Ты - интеллектуальный ассистент, который помогает пользователям находить информацию и отвечать на их вопросы. \n",
    "            Используй всю информацию из контекста.\n",
    "            Пиши без лишней информации.\n",
    "            Если ответ не может быть выведен из контекста, не отвечайте.\"\"\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"Контекст: {context}\n",
    "            ---\n",
    "            Вопрос, на который нужно ответить: {question}\"\"\"\n",
    "                    ),\n",
    "    ]\n",
    "    # print(f\"Контекст: {context} --- Вопрос, на который нужно ответить: {question}\")\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({})\n",
    "\n",
    "    answer += f'\\n\\nБолее подробную информацию вы можете найти по ссылкам: '\n",
    "    for i, x in enumerate(relevant_unique_):\n",
    "        if i != len(relevant_unique_)-1:\n",
    "            answer += x.metadata['url'] + ', '\n",
    "        else:\n",
    "            answer += x.metadata['url']\n",
    "\n",
    "    return answer, [x.metadata['url'] for x in relevant_unique_]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "60d1a219-df52-4511-a4f4-9ce6d56a94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "def fusion_questions(\n",
    "    question: str,\n",
    "    llm: Pipeline,\n",
    "    num_questions: int = 2,\n",
    ") -> List[str]:\n",
    "\n",
    "    llm = ChatOllama(model=\"aya:8b\")\n",
    "\n",
    "    questions = [question]\n",
    "\n",
    "    for _ in range(num_questions):\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"\"\"Ты интеллектуальный помощник, улучшающий чатбот техподдержки. \n",
    "                Твоя задача генирировать альтернативные формулировки вопроса пользователя.\n",
    "                Пиши только сгенерированный запрос, без другого текста.\"\"\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"\"\"Вопрос, который нужно переформулировать: {question}\"\"\"),\n",
    "        ]\n",
    "    \n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        answer = chain.invoke({})\n",
    "\n",
    "        questions.append(answer)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "341a91ce-2020-4352-bef7-c3f5a4c72dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='aya:8b')\n",
    "llm = ChatOllama(model=\"aya:8b\")\n",
    "# RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert/\")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = Qdrant.from_documents(\n",
    "    docs_processed,\n",
    "    embeddings,\n",
    "    path=\"./local_qdrant\",\n",
    "    collection_name=\"my_documents\",\n",
    ")\n",
    "\n",
    "# KNOWLEDGE_VECTOR_DATABASE = Qdrant.from_existing_collection(embeddings, path='./local_qdrant', collection_name=\"my_documents\")\n",
    "\n",
    "# # initialize the bm25 retriever and faiss retriever\n",
    "# bm25_retriever = BM25Retriever.from_documents(\n",
    "#     docs_processed\n",
    "# )\n",
    "# bm25_retriever.k = 5\n",
    "\n",
    "# with open('bm25result', 'wb') as bm25result_file:\n",
    "#     pickle.dump(bm25_retriever, bm25result_file)\n",
    "# # with open('bm25result', 'rb') as bm25result_file:\n",
    "# #     bm25_retriever = pickle.load(bm25result_file)\n",
    "\n",
    "# embedding = OllamaEmbeddings(model='aya:8b')\n",
    "# faiss_vectorstore = FAISS.from_documents(\n",
    "#     docs_processed, embedding\n",
    "# )\n",
    "# with open('faissresult', 'wb') as faissresult_file:\n",
    "#     pickle.dump(faiss_vectorstore, faissresult_file)\n",
    "# # with open('faissresult', 'rb') as faissresult_file:\n",
    "# #     faiss_vectorstore = pickle.load(faissresult_file)\n",
    "\n",
    "# faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# # initialize the ensemble retriever\n",
    "# ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[\n",
    "#         # bm25_retriever, \n",
    "#                 faiss_retriever], weights=[0.5, 0.5]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef00563-a3d6-4337-9691-547819d0a29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d8e0a2d9-8d7b-4b58-a03c-3319a47caaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Как посмотреть лимиты счета?', 'Каким образом я могу узнать лимиты своего счета?', 'Как проверить лимиты моего аккаунта?', 'Как проверить лимиты моего аккаунта?', '\"Как проверить лимиты моего аккаунта?\"']\n",
      "\n",
      "=> Retrieving documents...\n",
      "=> Unique documents: 7\n",
      "1\n",
      "['Вы можете посмотреть лимиты по счету на месяц, по карте на месяц и на разовую выплату. Для этого: Откройте раздел «Сводка». В разделе «Счета» найдите нужное вам юридическое лицо и нажмите на него. В открывшемся окне будут представлены все расчетные счета, к которым есть доступ с этого юридического лица. Найдите нужный вам счет.']\n",
      "=> Generating answer...\n"
     ]
    }
   ],
   "source": [
    "# question = \"Как настроить интеграцию с кассой АТОЛ?\"\n",
    "question = \"Как посмотреть лимиты счета?\"\n",
    "\n",
    "answer, relevant_docs = answer_with_rag(\n",
    "    question, llm, KNOWLEDGE_VECTOR_DATABASE, \n",
    "    # reranker=RERANKER,\n",
    "    num_docs_final=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dda760ba-d3eb-4d32-a7bf-b016bcb3a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Answer==================================\n",
      "Для проверки лимитов своего аккаунта необходимо выполнить следующие шаги: \n",
      "\n",
      "- Откройте раздел \"Сводка\" в своем персональном кабинете. \n",
      "- В разделе \"Счета\" выберите нужное юридическое лицо, используя соответствующую опцию. \n",
      "- После выбора юридического лица вы увидите список всех счетов, связанных с этим лицом. \n",
      "- Найдите нужный счет в списке и откройте его подробную информацию. \n",
      "- В информации о счете будут представлены сведения о лимитах, доступных для этого конкретного аккаунта. \n",
      "\n",
      "Эти инструкции позволяют быстро проверить текущие лимиты вашего аккаунта и убедиться, что вы не превысили их.\n",
      "\n",
      "Более подробную информацию вы можете найти по ссылкам: https://www.tinkoff.ru/business/help/business-payout/jump/about/summary/?card=q1\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================Answer==================================\")\n",
    "print(f\"{answer}\")\n",
    "# print(\"\\n==================================Source docs==================================\")\n",
    "# for i, doc in enumerate(relevant_docs):\n",
    "#     print(f\"Document {i}------------------------------------------------------------\")\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d472764-0584-40c7-824b-7fb6e5f188c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0f1c1ab3-d66f-441e-8cd6-7117a9a29605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec4cf9-9014-4474-8c67-c632b50674af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82651428-bef2-47eb-ba13-63c0edaac45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba46cd3-8158-4a15-a333-9f706cf4e140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac29ae21-0e09-4059-8a85-1574ad5a4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hare/opt/anaconda3/envs/whisper/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rag import ChatBotRAG\n",
    "\n",
    "# Example usage\n",
    "data_path = 'train_Tinkoff/dataset.json'\n",
    "vector_db_path = './local_qdrant'\n",
    "embeddings_model = 'aya:8b'\n",
    "llm_name = 'aya:8b'\n",
    "light_llm_name = 'aya:8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbea82c3-de4e-46a0-ae04-4edcce10350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBotRAG(data_path, vector_db_path, embeddings_model, llm_name, light_llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05b1bfb-461d-4f6f-a55f-685cc4b92258",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4665d3-f622-451a-8d49-0c907520564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                   | 209/3237 [04:05<1:27:00,  1.72s/it]"
     ]
    }
   ],
   "source": [
    "docs_processed = chatbot.split_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bf117-5fd9-42a5-8762-343efc997673",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.create_vector_database(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84727aa3-f537-4376-8e8f-f18056555a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Как настроить интеграцию с кассой АТОЛ?\"\n",
    "answer, url_links = chatbot.answer_with_rag(question, num_docs_final=5)\n",
    "\n",
    "print(\"==================================Answer==================================\")\n",
    "print(f\"{answer}\")\n",
    "print(\"\\n==================================Source docs==================================\")\n",
    "for i, url in enumerate(url_links):\n",
    "    print(f\"{i}) {url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74060fed-8c9c-4e71-8e37-8b8abc96de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)\n",
    "print()\n",
    "print(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc0513-206d-47ac-90ad-31212a582d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
